---
- hosts: controller_node
  become: true
  tasks:
    - name: Update apt package cache
      ansible.builtin.apt:
        update_cache: yes

    - name: Install dependencies
      ansible.builtin.apt:
        name:
          - libaio-dev
          - bison
          - cmake
          - libhttp-parser2.9
          - libhttp-parser-dev
          - libjson-c-dev
          - build-essential
          - libmunge-dev
          - libmysqlclient-dev
          - libssl-dev
          - pkg-config
          - mariadb-client
          - mariadb-server
          - libjwt-dev
          - libjansson-dev
          - libdbus-1-dev
          - linux-headers-{{ ansible_kernel }}
          - libreadline-dev
          - lua5.3
          - liblua5.3-dev
          - libhwloc-dev
          - libcurl4-openssl-dev
          - libpci-dev
          - libtool
          - autoconf
        state: present
      register: dependencies_install

- hosts: controller_node
  become: yes
  tasks:
    - name: Clone the http-parser repository
      git:
        repo: https://github.com/nodejs/http-parser.git
        dest: /tmp/http_parser
        version: v2.9.4
        depth: 1
        single_branch: yes

    - name: Build and install http-parser
      shell: |
        cd /tmp/http_parser
        make
        sudo make install
      args:
        creates: /usr/local/lib/libhttp_parser.so

- hosts: controller_node
  become: yes
  tasks:
    - name: Clone libjwt repository
      git:
        repo: "https://github.com/benmcollins/libjwt.git"
        dest: "/tmp/libjwt"
        version: "v1.12.0"
        depth: 1
        single_branch: yes

    - name: Run autoreconf
      command: autoreconf --force --install
      args:
        chdir: "/tmp/libjwt"

    - name: Configure libjwt
      command: ./configure --prefix=/usr/local
      args:
        chdir: "/tmp/libjwt"

    - name: Compile libjwt
      command: make -j
      args:
        chdir: "/tmp/libjwt"

    - name: Install libjwt
      command: make install
      args:
        chdir: "/tmp/libjwt"

- hosts: controller_node
  become: true
  tasks:
    - name: Clone Munge repository
      git:
        repo: "https://github.com/dun/munge.git"
        dest: "/home/bnguyen/Work/Munge"
        version: "master"
        update: yes
      tags: clone

    - name: List files in the Munge directory
      command: ls -la
      args:
        chdir: "/home/bnguyen/Work/Munge"

    - name: Run autoreconf (if necessary)
      command: autoreconf -i
      args:
        chdir: "/home/bnguyen/Work/Munge"

    - name: Configure Munge
      command: "./configure"
      args:
        chdir: "/home/bnguyen/Work/Munge"

    - name: Compile Munge (make)
      command: "make"
      args:
        chdir: "/home/bnguyen/Work/Munge"

    - name: Install Munge (make install)
      command: "make install"
      args:
        chdir: "/home/bnguyen/Work/Munge"
      become: true

- hosts: controller_node
  become: true
  tasks:
    - name: Create munge group
      group:
        name: munge
        state: present

    - name: Create munge user
      user:
        name: munge
        group: munge
        home: /var/lib/munge
        shell: /sbin/nologin
        system: yes
        state: present

- hosts: controller_node
  become: true
  tasks:
    - name: Set ownership and permissions on /etc/munge
      file:
        path: /etc/munge
        owner: munge
        group: munge
        mode: '0700'
        recurse: yes

    - name: Ensure /var/log is owned by root and has mode 0755
      file:
        path: /var/log
        owner: root
        group: root
        mode: '0755'

    - name: Create /var/log/munge directory with correct permissions
      file:
        path: /var/log/munge
        state: directory
        owner: munge
        group: munge
        mode: '0750'

    - name: Create /usr/local/var/log/munge with correct permissions
      file:
        path: /usr/local/var/log/munge
        state: directory
        owner: munge
        group: munge
        mode: '0750'

    - name: Create /usr/local/var/run with correct permissions
      file:
        path: /usr/local/var/run
        state: directory
        owner: munge
        group: munge
        mode: '0755'

    - name: Create /usr/local/var/run/munge with correct permissions
      file:
        path: /usr/local/var/run/munge
        state: directory
        owner: munge
        group: munge
        mode: '0755'

- hosts: controller_node
  become: true
  tasks:
    - name: Generate Munge key if it doesn't exist
      command: /usr/local/sbin/mungekey --verbose
      args:
        creates: /usr/local/etc/munge/munge.key

    - name: Set correct owner and permissions on Munge Directory
      file:
        path: /usr/local/etc/munge
        owner: munge
        group: munge
        mode: '0750'

    - name: Set correct owner and permissions on Munge key
      file:
        path: /usr/local/etc/munge/munge.key
        owner: munge
        group: munge
        mode: '0600'

    - name: Update ExecStart in munge systemd unit
      become: true
      ansible.builtin.lineinfile:
        path: /usr/local/lib/systemd/system/munge.service
        regexp: '^ExecStart='
        line: 'ExecStart=/usr/local/sbin/munged --socket=/run/munge/munge.socket.2'
        backup: yes

    - name: Reload systemd daemon
      become: true
      ansible.builtin.command:
        cmd: systemctl daemon-reexec

    - name: Enable and start Munge service
      ansible.builtin.systemd:
        name: munge
        enabled: yes
        state: started

    - name: Check Munge service status
      command: systemctl status munge.service
      register: munge_service_status
      ignore_errors: yes

    - name: Display Munge service status output
      debug:
        var: munge_service_status.stdout

- hosts: controller_node
  gather_facts: false
  become: true
  tasks:
    - name: Copy /usr/local/etc/munge/munge.key to home directory
      copy:
        src: /usr/local/etc/munge/munge.key
        dest: /home/bnguyen/munge.key
        mode: '0777'
        remote_src: true

#- hosts: controller_node
#  gather_facts: false
#  become: true
#  tasks:
#    - name: Copy munge.key to remote host using scp
#      shell: >
#        scp -o StrictHostKeyChecking=no /home/bnguyen/munge.key
#        'billy.nguyen@sentientvision.com@192.168.91.107:~/'

- hosts: controller_node
  gather_facts: false
  become: true
  tasks:
    - name: Remove munge.key from home directory
      file:
        path: /home/bnguyen/munge.key
        state: absent

- hosts: compute_nodes
  become: true
  tasks:
    - name: Move munge.key to /etc/munge/
      command: mv /home/billy.nguyen@sentientvision.com/munge.key /usr/local/etc/munge/munge.key
      args:
        removes: /home/billy.nguyen@sentientvision.com/munge.key

    - name: Change ownership to munge:munge
      file:
        path: /usr/local/etc/munge/munge.key
        owner: munge

    - name: Set permissions for Munge.key
      file:
        path: /usr/local/etc/munge/munge.key
        mode: '0600'

- hosts: controller_node
  become: true
  tasks:
    - name: Clone Slurm repository (slurm-24.11 branch)
      git:
        repo: "https://github.com/SchedMD/slurm.git"
        dest: "/home/bnguyen/Work/Slurm"
        version: "slurm-24.11"
        update: yes
      tags: clone

    - name: List files in the Slurm directory
      command: ls -la
      args:
        chdir: "/home/bnguyen/Work/Slurm"

    - name: Run autoreconf (if necessary)
      command: autoreconf -i
      args:
        chdir: "/home/bnguyen/Work/Slurm"
      when: "'configure' not in lookup('fileglob', '/home/bnguyen/Work/Slurm/*')"

    - name: Configure Slurm
      command: "./configure --with-jwt=/usr/local/ --with-bpf --with-http-parser=/usr/local/"
      args:
        chdir: "/home/bnguyen/Work/Slurm"

    - name: Install Slurm (make install)
      command: "make install"
      args:
        chdir: "/home/bnguyen/Work/Slurm"
      become: true


- hosts: controller_node
  become: true
  tasks:
    - name: Create slurm group
      group:
        name: slurm
        state: present

    - name: Create slurm user
      user:
        name: slurm
        group: slurm
        home: /var/lib/slurm
        shell: /sbin/nologin
        system: yes
        state: present

- hosts: controller_node
  become: yes
  tasks:
    - name: Create Slurm configuration file
      ansible.builtin.copy:
        dest: "/usr/local/etc/slurm.conf"
        content: |
          # Slurm configuration goes here
          # slurm.conf file generated by configurator.html.
          # Put this file on all nodes of your cluster.
          # See the slurm.conf man page for more information.
          #
          ClusterName=cluster
          ControlMachine=BM-Ubuntu24
          SlurmctldParameters=enable_configless,job_submit_plugins=/usr/local/etc/job_submit.lua
          #SlurmctldHost=DEV-DSKTP-BillyN          
          #SlurmctldHost=
          #
          AuthType=auth/munge
          AuthInfo=/var/run/munge/munge.socket.2
          AuthAltTypes=auth/jwt
          AuthAltParameters=jwt_key=/var/spool/slurm/statesave/jwt_hs256.key
          #DisableRootJobs=NO
          #EnforcePartLimits=NO
          #Epilog=
          #EpilogSlurmctld=
          #FirstJobId=1
          #MaxJobId=67043328
          #GresTypes=   
          #GroupUpdateForce=0
          #GroupUpdateTime=600
          #JobFileAppend=0
          #JobRequeue=1
          JobSubmitPlugins=lua
          #KillOnBadExit=0
          #LaunchType=launch/slurm
          #Licenses=foo*4,bar
          MailProg=/bin/mail
          #MaxJobCount=10000
          #MaxStepCount=40000
          #MaxTasksPerNode=512
          #MpiDefault=
          #MpiParams=ports=#-#
          #PluginDir=
          #PlugStackConfig=
          #PrivateData=jobs
          ProctrackType=proctrack/cgroup
          #Prolog=
          PrologFlags=X11
          #PrologSlurmctld=
          #PropagatePrioProcess=0
          #PropagateResourceLimits=
          #PropagateResourceLimitsExcept=
          #RebootProgram=
          ReturnToService=1
          SlurmctldPidFile=/var/run/slurmctld.pid
          SlurmctldPort=6817
          SlurmdPidFile=/var/run/slurmd.pid
          SlurmdPort=6818
          SlurmdSpoolDir=/var/spool/slurmd
          SlurmUser=slurm
          #SlurmdUser=root
          #SrunEpilog=
          #SrunProlog=
          StateSaveLocation=/var/spool/slurmctld
          #SwitchType=
          #TaskEpilog=
          TaskPlugin=task/affinity,task/cgroup
          #TaskProlog=
          #TopologyPlugin=topology/tree
          #TmpFS=/tmp
          #TrackWCKey=no
          #TreeWidth=
          #UnkillableStepProgram=
          #UsePAM=0
          #
          #
          # TIMERS
          BatchStartTimeout=11
          #CompleteWait=0
          #EpilogMsgTime=2000
          #GetEnvTimeout=2
          #HealthCheckInterval=0
          #HealthCheckProgram=
          InactiveLimit=0
          KillWait=30
          #MessageTimeout=10
          #ResvOverRun=0
          MinJobAge=300
          #OverTimeLimit=0
          SlurmctldTimeout=120
          SlurmdTimeout=300
          #UnkillableStepTimeout=60
          #VSizeFactor=0
          Waittime=0
          #
          #
          # SCHEDULING
          #DefMemPerCPU=0
          #MaxMemPerCPU=0
          #SchedulerTimeSlice=30
          SchedulerType=sched/backfill
          SelectType=select/cons_tres
          #
          #
          # JOB PRIORITY
          #PriorityFlags=
          #PriorityType=priority/multifactor
          #PriorityDecayHalfLife=
          #PriorityCalcPeriod=
          #PriorityFavorSmall=
          #PriorityMaxAge=
          #PriorityUsageResetPeriod=
          #PriorityWeightAge=
          #PriorityWeightFairshare=
          #PriorityWeightJobSize=
          #PriorityWeightPartition=
          #PriorityWeightQOS=
          #
          #
          # LOGGING AND ACCOUNTING
          #AccountingStorageEnforce=0
          AccountingStorageHost=BM-Ubuntu24
          AccountingStoragePort=6819
          AccountingStorageType=accounting_storage/slurmdbd
          AccountingStorageUser=slurm
          #AccountingStoreFlags=
          JobCompType=jobcomp/mysql
          JobCompHost=BM-Ubuntu24
          JobCompPort=3306
          JobCompUser=slurm
          JobCompPass=password
          JobCompLoc=slurm_acct_db
          #JobCompParams=
          #JobContainerType=
          JobAcctGatherFrequency=30
          #JobAcctGatherType=
          SlurmctldDebug=info
          SlurmctldLogFile=/var/log/slurmctld.log
          SlurmdDebug=info
          SlurmdLogFile=/var/log/slurmd.log
          #SlurmSchedLogFile=
          #SlurmSchedLogLevel=
          #DebugFlags=
          #
          #
          # POWER SAVE SUPPORT FOR IDLE NODES (optional)
          #SuspendProgram=
          #ResumeProgram=
          #SuspendTimeout=
          #ResumeTimeout=
          #ResumeRate=
          #SuspendExcNodes=
          #SuspendExcParts=
          #SuspendRate=
          #SuspendTime=
          #
          #
          # COMPUTE NODES
          NodeName=BM-Ubuntu24 CPUs=16 RealMemory=31983 Sockets=1 CoresPerSocket=8 ThreadsPerCore=2 State=UNKNOWN
          NodeName=dl-testnode CPUs=16 RealMemory=64215 Sockets=1 CoresPerSocket=8 ThreadsPerCore=2 State=UNKNOWN
          
          # Define partitions
          PartitionName=DLJarvis Nodes=BM-Ubuntu24 Default=YES MaxTime=INFINITE State=UP
          PartitionName=DLInteractive Nodes=BM-Ubuntu24 Default=NO MaxTime=00:02:00 State=UP
          PartitionName=DLJarvisSmall Nodes=BM-Ubuntu24 Default=NO MaxTime=INFINITE State=UP
          PartitionName=DLJarvisSmallInteractive Nodes=BM-Ubuntu24 Default=NO MaxTime=00:02:00 State=UP
          PartitionName=test Nodes=dl-testnode Default=NO MaxTime=INFINITE State=UP
        mode: '0644'

- hosts: controller_node
  become: true
  tasks:
    - name: Ensure slurm user exists with correct home and shell
      user:
        name: slurm
        home: /var/lib/slurm
        shell: /bin/bash
        create_home: yes
        system: yes
        state: present

    - name: Create necessary directories
      file:
        path: "{{ item }}"
        state: directory
        owner: slurm
        group: slurm
        mode: '0700'
      loop:
        - /var/spool/slurmctld
        - /var/spool/slurmd
        - /var/spool/slurm

    - name: Create necessary files
      file:
        path: "{{ item }}"
        state: touch
        owner: slurm
        group: slurm
      loop:
        - /var/log/slurmd.log
        - /var/run/slurmdbd.pid
        - /var/log/slurmdbd.log

    - name: Set correct ownership for Slurm files and directories
      file:
        path: "{{ item.path }}"
        owner: slurm
        group: slurm
        mode: "{{ item.mode | default(omit) }}"
        recurse: "{{ item.recurse | default(omit) }}"
      loop:
        - { path: "/usr/local/etc/slurm.conf", mode: "0644" }
        - { path: "/var/lib/slurm", recurse: yes }
        - { path: "/var/spool/slurmctld", recurse: yes }
        - { path: "/var/spool/slurm", recurse: yes }
        - { path: "/var/log/slurmd.log", mode: "0644" }
        - { path: "/var/spool/slurmd", recurse: yes }
        - { path: "/var/log/slurmdbd.log", mode: "0644" }

    - name: Set permissions for /var/spool/slurmd
      file:
        path: /var/spool/slurmd
        mode: '0700'
        recurse: yes

    - name: Copy systemd service files
      copy:
        src: "/home/bnguyen/Work/Slurm/etc/{{ item }}"
        dest: "/etc/systemd/system/{{ item }}"
        mode: '0644'
      loop:
        - slurmd.service
        - slurmctld.service
        - slurmdbd.service

- hosts: controller_node
  become: yes
  tasks:
    - name: Create Slurm Database configuration file
      ansible.builtin.copy:
        dest: "/usr/local/etc/slurmdbd.conf"
        content: |
          ArchiveEvents=yes
          ArchiveJobs=yes
          ArchiveResvs=yes
          ArchiveSteps=no
          ArchiveSuspend=no
          ArchiveTXN=no
          ArchiveUsage=no
          #ArchiveScript=/usr/sbin/slurm.dbd.archive
          AuthInfo=/var/run/munge/munge.socket.2
          AuthType=auth/munge
          AuthAltTypes=auth/jwt
          AuthAltParameters=jwt_key=/var/spool/slurm/statesave/jwt_hs256.key
          DbdHost=BM-Ubuntu24
          DbdPort=6819
          DebugLevel=info
          PurgeEventAfter=1month
          PurgeJobAfter=12month
          PurgeResvAfter=1month
          PurgeStepAfter=1month
          PurgeSuspendAfter=1month
          PurgeTXNAfter=12month
          PurgeUsageAfter=24month
          LogFile=/var/log/slurmdbd.log
          PidFile=/var/run/slurmdbd.pid
          SlurmUser=slurm
          StoragePass=password
          StorageType=accounting_storage/mysql
          StorageUser=slurm
          StorageHost=BM-Ubuntu24
        mode: '0644'

- hosts: controller_node
  become: true
  tasks:
    - name: Set correct ownership and permissions for Slurm Database files
      file:
        path: "{{ item.path }}"
        owner: slurm
        group: slurm
        mode: "{{ item.mode }}"
      loop:
        - { path: "/usr/local/etc/slurmdbd.conf", mode: "0600" }
        - { path: "/var/run/slurmdbd.pid", mode: "0644" }


- hosts: controller_node
  become: yes
  tasks:
    - name: Create Cgroup configuration file
      ansible.builtin.copy:
        dest: "/usr/local/etc/cgroup.conf"
        content: |
          ConstrainCores=yes
          ConstrainDevices=yes
          # TaskAffinity=yes
          ConstrainRAMSpace=yes
          # ConstrainSwapSpace=yes
          MaxRAMPercent=98
          AllowedSwapSpace=0
          AllowedRAMSpace=100
          MemorySwappiness=0
        mode: '0644'

- hosts: controller_node
  become: yes
  tasks:
    - name: Create Lua Script
      ansible.builtin.copy:
        dest: "/usr/local/etc/job_submit.lua"
        content: |
          function slurm_job_submit(job_desc, part_list, submit_uid)
              local log_prefix = "slurm_job_submit"
              local default_partition = "Large"
              local interactive_suffix = "Interactive"

              -- Set default partition if not specified
              if job_desc.partition == nil or job_desc.partition == '' then
                  job_desc.partition = default_partition
                  slurm.log_info("%s: No partition specified, defaulting to %s.", log_prefix, job_desc.partition)
              end

              -- Check if the job is interactive (srun)
              if job_desc.script == nil or job_desc.script == '' then
                  -- Check if the current partition is Large or Small
                  if job_desc.partition == "Large" or job_desc.partition == "Small" then
                      -- Append "Interactive" to the partition name
                      job_desc.partition = job_desc.partition .. interactive_suffix
                      slurm.log_info("%s: Interactive job detected, moved to %s partition.", log_prefix, job_desc.partition)
                  end
              end

              slurm.log_info("%s: For user %u, setting partition: %s.", log_prefix, submit_uid, job_desc.partition)
              slurm.log_user("Job \"%s\" queued to partition: %s.", job_desc.name, job_desc.partition)

              return slurm.SUCCESS
          end

          function slurm_job_modify(job_desc, job_rec, part_list, modify_uid)
              return slurm.SUCCESS
          end

          slurm.log_info("initialized")
          return slurm.SUCCESS
        mode: '0644'

    - name: Generate JWT HS256 key
      command: openssl rand -base64 32
      register: jwt_key

    - name: Create directory for JWT key file
      file:
        path: /var/spool/slurm/statesave
        state: directory
        mode: '0755'
        owner: slurm
        group: slurm

    - name: Write JWT HS256 key to file
      copy:
        content: "{{ jwt_key.stdout }}"
        dest: /var/spool/slurm/statesave/jwt_hs256.key
        owner: slurm
        group: slurm
        mode: '0600'

- hosts: controller_node
  become: true
  tasks:
    - name: Enable and start Slurm services
      systemd:
        name: "{{ item }}"
        enabled: yes
        state: started
      loop:
        - slurmctld
        - slurmd
        - slurmdbd

    - name: Check Slurm services status
      command: systemctl status "{{ item }}"
      register: slurm_service_status
      ignore_errors: yes
      loop:
        - slurmctld
        - slurmd
        - slurmdbd

    - name: Display Slurm services status output
      debug:
        var: slurm_service_status.results
